
import streamlit as st
import pandas as pd
import numpy as np
from datetime import timedelta
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.naive_bayes import GaussianNB
import shap
import matplotlib.pyplot as plt
from sklearn.inspection import PartialDependenceDisplay
import plotly.graph_objects as go
from plotly.subplots import make_subplots

import matplotlib
import platform

# --- í•œê¸€ í°íŠ¸ ì„¤ì • ---
st.markdown("""
<style>
/* êµ¬ê¸€ Noto Sans KR í°íŠ¸ ë¡œë“œ */
@import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');

html, body, [class*="css"]  {
    font-family: 'Noto Sans KR', sans-serif;
}
</style>
""", unsafe_allow_html=True)

st.set_page_config(layout="wide")
st.title("ìŠ¤ë§ˆíŠ¸íŒœ ìˆ˜í™•ëŸ‰ + ìƒìœ¡ ì˜ˆì¸¡ XAI í†µí•© ëŒ€ì‹œë³´ë“œ")

import streamlit as st
import pandas as pd
import numpy as np
from datetime import timedelta

# -------------------------------------------------------------
# ì‘ë¬¼ì„ íƒ
# -------------------------------------------------------------
crop_name = st.selectbox("ì‘ë¬¼ ì„ íƒ", ["í† ë§ˆí† ", "ì˜¤ì´"])

# -------------------------------------------------------------
# íŒŒì¼ ì—…ë¡œë“œ
# -------------------------------------------------------------
sensor_file = st.file_uploader("í™˜ê²½ì„¼ì„œ ë°ì´í„° ì—…ë¡œë“œ (CSV)", type=["csv"])
yield_file = st.file_uploader("ìˆ˜í™•/ìƒìœ¡ ë°ì´í„° ì—…ë¡œë“œ (CSV)", type=["csv"])

if sensor_file and yield_file:
    sensor_df = pd.read_csv(sensor_file)
    yield_df = pd.read_csv(yield_file)

    st.subheader("í™˜ê²½ì„¼ì„œ ë°ì´í„°")
    st.dataframe(sensor_df.head())

    st.subheader("ìˆ˜í™•/ìƒìœ¡ ë°ì´í„°")
    st.dataframe(yield_df.head())

    # -------------------------------------------------------------
    # í™˜ê²½ ì„¼ì„œ ì»¬ëŸ¼ ì„ íƒ (ê°€ë¡œ 5ê°œ)
    # -------------------------------------------------------------
    st.subheader("ì»¬ëŸ¼ ì„ íƒ")
    st.markdown("**í™˜ê²½ ì„¼ì„œ ë°ì´í„° ì»¬ëŸ¼ ì„ íƒ**")

    col1, col2, col3, col4, col5 = st.columns(5)

    with col1:
        date_col_sensor = st.selectbox("ë‚ ì§œì‹œê°„", sensor_df.columns)
    with col2:
        temp_col = st.selectbox("ì˜¨ë„", sensor_df.columns)
    with col3:
        hum_col = st.selectbox("ìŠµë„", sensor_df.columns)
    with col4:
        co2_col = st.selectbox("COâ‚‚", sensor_df.columns)
    with col5:
        solar_col = st.selectbox("ì¼ì‚¬ëŸ‰", sensor_df.columns)

    st.markdown("---")

    # -------------------------------------------------------------
    # ìˆ˜í™•ëŸ‰ ì»¬ëŸ¼ ì„ íƒ (ê°€ë¡œ 3ê°œ)
    # -------------------------------------------------------------
    st.markdown("**ìˆ˜í™•ëŸ‰ ë°ì´í„° ì»¬ëŸ¼ ì„ íƒ**")

    col6, col7, col8 = st.columns(3)

    with col6:
        date_col_yield = st.selectbox("ì¡°ì‚¬ì¼ì", yield_df.columns)
    with col7:
        harvest_count_col = st.selectbox("ìˆ˜í™•ìˆ˜", yield_df.columns)
    with col8:
        harvest_weight_col = st.selectbox("í‰ê· ê³¼ì¤‘", yield_df.columns)

    st.markdown("---")

    # -------------------------------------------------------------
    # ì‘ë¬¼ë³„ ìƒìœ¡ ì»¬ëŸ¼ ì„ íƒ
    # -------------------------------------------------------------
    st.markdown("**ì¶”ê°€ ìƒìœ¡ ì»¬ëŸ¼ ì„ íƒ**")

    if crop_name == "í† ë§ˆí† ":
        growth_features = ["ì´ˆì¥", "ìƒì¥ê¸¸ì´", "ì—½ìˆ˜", "ì—½ì¥", "ì—½í­", "ì¤„ê¸°êµµê¸°", "í™”ë°©ë†’ì´"]
    else:   # ì˜¤ì´
        growth_features = ["ì´ˆì¥", "ì—½ìˆ˜", "ì—½ì¥", "ì—½í­", "ì¤„ê¸°êµµê¸°", "í™”ë°©ë†’ì´"]

    growth_cols = {}

    # 3ê°œì”© ëŠì–´ì„œ UI ê°€ë¡œ ë°°ì¹˜
    for i in range(0, len(growth_features), 3):
        cols = st.columns(3)
        for j, gf in enumerate(growth_features[i:i + 3]):
            with cols[j]:
                # ì‚¬ìš©ìê°€ yield_dfì˜ ì»¬ëŸ¼ì„ ë§¤í•‘í•˜ë„ë¡ í•¨(ì—†ìœ¼ë©´ None)
                options = [None] + yield_df.columns.tolist()
                if gf in yield_df.columns:
                    default_idx = yield_df.columns.get_loc(gf) + 1
                else:
                    default_idx = 0
                growth_cols[gf] = st.selectbox(f"{gf}", options, index=default_idx)

        # -------------------------------------------------------------
        # ë‚ ì§œ ë³€í™˜ (ì•ˆì „í•˜ê²Œ)
        # -------------------------------------------------------------
        sensor_df[date_col_sensor] = pd.to_datetime(sensor_df[date_col_sensor], errors='coerce')
        yield_df[date_col_yield] = pd.to_datetime(yield_df[date_col_yield], errors='coerce')

        # ë³€í™˜ ì‹¤íŒ¨(NaT) ì œê±°
        sensor_df = sensor_df.dropna(subset=[date_col_sensor])
        yield_df = yield_df.dropna(subset=[date_col_yield])

        # date, hour, time ì»¬ëŸ¼ ìƒì„±
        sensor_df["date"] = sensor_df[date_col_sensor].dt.date
        sensor_df["hour"] = sensor_df[date_col_sensor].dt.hour
        sensor_df["time"] = sensor_df[date_col_sensor].dt.time

    # --- ì£¼ ì„ íƒ ìŠ¬ë¼ì´ë” ë™ê¸°í™” ---
    if "weeks" not in st.session_state:
        st.session_state.weeks = 7  # ì´ˆê¸°ê°’


    def update_weeks_1():
        st.session_state.weeks = st.session_state.weeks_slider_1


    def update_weeks_2():
        st.session_state.weeks = st.session_state.weeks_slider_2


    weeks1 = st.slider("í‰ê·  ê³„ì‚° ê¸°ê°„ (ì£¼ ë‹¨ìœ„) - ì„¼ì„œ í‰ê· ìš©",
                       1, 7, st.session_state.weeks, key="weeks_slider_1", on_change=update_weeks_1)
    days = st.session_state.weeks * 7

    # í‘œì¤€í™”ëœ ë™ì  ì»¬ëŸ¼ëª… (days ê¸°ë°˜)
    temp_col_name = f"{days}ì¼í‰ê· ì˜¨ë„(24ì‹œê°„)"
    hum_col_name = f"{days}ì¼í‰ê· ìŠµë„(08~18ì‹œ)"
    co2_col_name = f"{days}ì¼í‰ê· COâ‚‚(06~18ì‹œ)"
    solar_col_name = f"{days}ì¼í‰ê· ëˆ„ì ì¼ì‚¬ëŸ‰(0:00ê¸°ì¤€)"

    # -------------------------------------------------------------
    # ë§¤í•‘ ê³„ì‚° (yield_df í–‰ë§ˆë‹¤ sensor ë°ì´í„°ë¡œ íŒŒìƒë³€ìˆ˜ ìƒì„±)
    # -------------------------------------------------------------
    results = []

    for idx, row in yield_df.iterrows():
        date = row[date_col_yield]
        start_date = date - timedelta(days=days)

        mask = (sensor_df[date_col_sensor] >= start_date) & (sensor_df[date_col_sensor] <= date)
        subset = sensor_df.loc[mask]

        # ì´ˆê¸°ê°’ None (ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ None ìœ ì§€)
        avg_solar = None
        avg_co2 = None
        avg_temp = None
        avg_hum = None

        if not subset.empty:
            # --- ì¼ì‚¬ëŸ‰(0ì‹œ ê¸°ì¤€): ê° dateì˜ 00:00 ë ˆì½”ë“œì—ì„œ ì¼ì‚¬ëŸ‰ ì¶”ì¶œ í›„ ê·¸ ë‚ ì§œë“¤ í‰ê· 
            midnight_values = subset[subset["time"].astype(str) == "00:00:00"]
            if not midnight_values.empty:
                midnight_daily = midnight_values.groupby("date")[solar_col].first().reset_index()
                if not midnight_daily.empty:
                    avg_solar = midnight_daily[solar_col].mean()

            # --- CO2 (06~18ì‹œ)
            co2_daytime = subset[(subset["hour"] >= 6) & (subset["hour"] <= 18)]
            if not co2_daytime.empty:
                co2_daily_mean = co2_daytime.groupby("date")[co2_col].mean().reset_index()
                if not co2_daily_mean.empty:
                    avg_co2 = co2_daily_mean[co2_col].mean()

            # --- ì˜¨ë„ (24ì‹œê°„ í‰ê· )
            if temp_col in subset.columns:
                avg_temp = subset[temp_col].mean()

            # --- ìŠµë„ (08~18ì‹œ)
            hum_daytime = subset[(subset["hour"] >= 8) & (subset["hour"] <= 18)]
            if not hum_daytime.empty and hum_col in hum_daytime.columns:
                avg_hum = hum_daytime[hum_col].mean()

        # ê²°ê³¼ í–‰ì— ë°˜ë“œì‹œ ë™ì  ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì €ì¥ (ì¼ê´€ì„± í™•ë³´)
        result_row = {
            "ì¡°ì‚¬ì¼ì": date,
            "ìˆ˜í™•ìˆ˜": row[harvest_count_col] if harvest_count_col in row else None,
            "í‰ê· ê³¼ì¤‘": row[harvest_weight_col] if harvest_weight_col in row else None,
            temp_col_name: avg_temp,
            hum_col_name: avg_hum,
            co2_col_name: avg_co2,
            solar_col_name: avg_solar
        }

        # ìƒìœ¡ ì»¬ëŸ¼ ì¶”ê°€ (ì‚¬ìš©ìê°€ ë§¤í•‘í•œ ì»¬ëŸ¼ëª…ì—ì„œ ê°’ ì¶”ì¶œ)
        for gf, col in growth_cols.items():
            if col and col in row.index:
                result_row[gf] = row[col]
            else:
                result_row[gf] = None

        results.append(result_row)

    df = pd.DataFrame(results)

    st.subheader("ë§¤í•‘ ë°ì´í„°")
    st.dataframe(df)

    # -------------------------------------------------------------
    # í™˜ê²½ ì»¬ëŸ¼ ë§¤í•‘ (ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ë¼ë²¨ â†’ ì‹¤ì œ ì»¬ëŸ¼ëª…)
    # -------------------------------------------------------------
    env_mapping = {
        f"{days}ì¼í‰ê· ì˜¨ë„(24ì‹œê°„)": temp_col_name,
        f"{days}ì¼í‰ê· ìŠµë„(08~18ì‹œ)": hum_col_name,
        f"{days}ì¼í‰ê· COâ‚‚(06~18ì‹œ)": co2_col_name,
        f"{days}ì¼í‰ê· ëˆ„ì ì¼ì‚¬ëŸ‰(0:00ê¸°ì¤€)": solar_col_name
    }

    env_cols = st.multiselect(
        "í™˜ê²½ ê·¸ë˜í”„ë¡œ í‘œì‹œí•  í•­ëª© ì„ íƒ",
        list(env_mapping.keys()),
        default=list(env_mapping.keys())
    )

    # -------------------------------------------------------------
    # í™˜ê²½ ê·¸ë˜í”„ ì¶œë ¥ (2ì—´ ë°°ì¹˜)
    # -------------------------------------------------------------
    if env_cols:
        for i in range(0, len(env_cols), 2):
            cols = st.columns(2)
            for j, label in enumerate(env_cols[i:i + 2]):
                with cols[j]:
                    true_col = env_mapping[label]
                    if true_col in df.columns:
                        fig, ax = plt.subplots(figsize=(5, 3))
                        ax.plot(df["ì¡°ì‚¬ì¼ì"], df[true_col], marker="o", linestyle="-")
                        ax.set_title(f"{label} ì‹œê³„ì—´")
                        ax.set_xlabel("ì¡°ì‚¬ì¼ì")
                        ax.set_ylabel(label)
                        ax.tick_params(axis='x', rotation=45)
                        ax.grid(True, linestyle="--", alpha=0.5)
                        st.pyplot(fig)
                        plt.close(fig)
                    else:
                        st.info(f"ì»¬ëŸ¼ '{true_col}'(ì‹¤ì œ ë°ì´í„°)ê°€ ì—†ì–´ '{label}' ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # --- ë‚ ì§œ ì •ë ¬
    df = df.sort_values("ì¡°ì‚¬ì¼ì")

    # -------------------------------------------------------------
    # ê·¸ë˜í”„ë¡œ í‘œì‹œí•  í•­ëª© ì„ íƒ(ìˆ˜í™•/ìƒìœ¡)
    # -------------------------------------------------------------
    growth_options = ["ìˆ˜í™•ìˆ˜", "í‰ê· ê³¼ì¤‘"] + growth_features

    plot_cols = st.multiselect(
        "ê·¸ë˜í”„ë¡œ í‘œì‹œí•  í•­ëª© ì„ íƒ",
        growth_options,
        default=["ìˆ˜í™•ìˆ˜", "í‰ê· ê³¼ì¤‘"]
    )

    if plot_cols:
        for i in range(0, len(plot_cols), 3):
            cols = st.columns(3)
            for j, col_name in enumerate(plot_cols[i:i + 3]):
                with cols[j]:
                    if col_name in df.columns:
                        fig, ax = plt.subplots(figsize=(4.5, 3))
                        ax.plot(df["ì¡°ì‚¬ì¼ì"], df[col_name], marker="o", linestyle="-")
                        ax.set_title(f"{col_name} ì‹œê³„ì—´")
                        ax.set_xlabel("ì¡°ì‚¬ì¼ì")
                        ax.set_ylabel(col_name)
                        ax.tick_params(axis='x', rotation=45)
                        ax.grid(True, linestyle="--", alpha=0.5)
                        st.pyplot(fig)
                        plt.close(fig)
                    else:
                        st.info(f"ì»¬ëŸ¼ '{col_name}' ê°€ ì—†ì–´ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # --- ğŸŒ¿ í™˜ê²½ vs ìƒìœ¡ 2ì¶• ì‹œê³„ì—´ ê·¸ë˜í”„ (4ê°œ ë¹„êµ) ---
    st.subheader("ğŸŒ¿ í™˜ê²½ vs ìƒìœ¡ 2ì¶• ì‹œê³„ì—´ ê·¸ë˜í”„ (4ê°œ ë¹„êµ)")

    # env_list: ì¢Œì¸¡ ë ˆì´ë¸”ê³¼ ì‹¤ì œ ì»¬ëŸ¼ëª…(ë™ì )
    env_list = [
        ("í‰ê· ì˜¨ë„", temp_col_name),
        ("í‰ê· ìŠµë„", hum_col_name),
        ("í‰ê· COâ‚‚", co2_col_name),
        ("í‰ê· ëˆ„ì ì¼ì‚¬ëŸ‰", solar_col_name)
    ]

    growth_choice = st.selectbox(
        "ìƒìœ¡ ì»¬ëŸ¼ ì„ íƒ (2ì¶• ê·¸ë˜í”„ì—ì„œ í‘œì‹œí•  í•­ëª©)",
        growth_options,
        index=0
    )

    for i in range(0, len(env_list), 2):
        cols = st.columns(2)
        for j, (title, col_name) in enumerate(env_list[i:i + 2]):
            with cols[j]:
                if col_name not in df.columns:
                    st.info(f"í™˜ê²½ ì»¬ëŸ¼ '{col_name}' ê°€ ì—†ì–´ '{title}' í”Œë¡¯ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
                if growth_choice not in df.columns:
                    st.info(f"ìƒìœ¡ ì»¬ëŸ¼ '{growth_choice}' ê°€ ì—†ì–´ ì˜¤ë¥¸ìª½ ì¶•ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                fig, ax1 = plt.subplots(figsize=(5.5, 3.5))
                # í™˜ê²½ (ì™¼ìª½)
                color1 = "tab:blue"
                ax1.set_xlabel("ì¡°ì‚¬ì¼ì")
                ax1.set_ylabel(title, color=color1)
                ax1.plot(df["ì¡°ì‚¬ì¼ì"], df[col_name], color=color1, marker="o", label=title)
                ax1.tick_params(axis='y', labelcolor=color1)
                ax1.tick_params(axis='x', rotation=45)
                ax1.grid(True, linestyle="--", alpha=0.4)
                # ìƒìœ¡ (ì˜¤ë¥¸ìª½) â€” only if exists
                ax2 = ax1.twinx()
                color2 = "tab:red"
                if growth_choice in df.columns:
                    ax2.plot(df["ì¡°ì‚¬ì¼ì"], df[growth_choice], color=color2, marker="s", linestyle="--", label=growth_choice)
                    ax2.set_ylabel(growth_choice, color=color2)
                    ax2.tick_params(axis='y', labelcolor=color2)
                ax1.legend(loc="best", fontsize=8)
                ax1.set_title(f"{title} vs {growth_choice}", fontsize=11)
                st.pyplot(fig)
                plt.close(fig)

    # --- Plotly interactive 2x2 ---
    st.subheader("ğŸŒ¿ í™˜ê²½ìš”ì†Œ vs ìƒìœ¡ì»¬ëŸ¼ 2ì¶• ì‹œê³„ì—´ ê·¸ë˜í”„ (Plotly ì¸í„°ë™í‹°ë¸Œ 2Ã—2)")

    growth_choice_plotly = st.selectbox(
        "ìƒìœ¡ ì»¬ëŸ¼ ì„ íƒ (Plotly ê·¸ë˜í”„ìš©)",
        growth_options,
        index=0,
        key="plotly_growth_choice"
    )

    # prepare subplot
    fig_plotly = make_subplots(
        rows=2, cols=2,
        subplot_titles=[f"{title} vs {growth_choice_plotly}" for title, _ in env_list],
        specs=[[{"secondary_y": True}, {"secondary_y": True}],
               [{"secondary_y": True}, {"secondary_y": True}]]
    )

    for idx, (title, env_col) in enumerate(env_list):
        row = idx // 2 + 1
        col = idx % 2 + 1

        if env_col in df.columns:
            fig_plotly.add_trace(
                go.Scatter(
                    x=df["ì¡°ì‚¬ì¼ì"],
                    y=df[env_col],
                    mode='lines+markers',
                    name=title,
                    line=dict(color='blue'),
                    hovertemplate=f"{title}: %{{y}}<br>ì¡°ì‚¬ì¼ì: %{{x}}"
                ), row=row, col=col, secondary_y=False
            )
        else:
            # add empty trace so subplot remains visible
            fig_plotly.add_trace(
                go.Scatter(x=[], y=[], name=f"{title} (no data)"),
                row=row, col=col, secondary_y=False
            )

        if growth_choice_plotly in df.columns:
            fig_plotly.add_trace(
                go.Scatter(
                    x=df["ì¡°ì‚¬ì¼ì"],
                    y=df[growth_choice_plotly],
                    mode='lines+markers',
                    name=growth_choice_plotly,
                    line=dict(color='red', dash='dash'),
                    hovertemplate=f"{growth_choice_plotly}: %{{y}}<br>ì¡°ì‚¬ì¼ì: %{{x}}"
                ), row=row, col=col, secondary_y=True
            )
        else:
            fig_plotly.add_trace(
                go.Scatter(x=[], y=[], name=f"{growth_choice_plotly} (no data)"),
                row=row, col=col, secondary_y=True
            )

        fig_plotly.update_yaxes(title_text=title, row=row, col=col, secondary_y=False)
        fig_plotly.update_yaxes(title_text=growth_choice_plotly, row=row, col=col, secondary_y=True)

    fig_plotly.update_layout(
        height=800,
        width=950,
        title_text="í™˜ê²½ìš”ì†Œ vs ìƒìœ¡ì»¬ëŸ¼ 2ì¶• ì‹œê³„ì—´ (ì¸í„°ë™í‹°ë¸Œ)",
        showlegend=True,
        hovermode="x unified",
        margin=dict(l=30, r=30, t=60, b=30)
    )

    st.plotly_chart(fig_plotly, use_container_width=True)

    # --- ëª¨ë¸ ì„ íƒ ---
    st.subheader("ëª¨ë¸ ì„ íƒ")
    model_options = ["RandomForest", "GradientBoosting", "XGBoost", "LGBM", "GaussianNB"]
    model_choice = st.selectbox("ëª¨ë¸ ì„ íƒ", model_options)

    target_col = st.selectbox("ì˜ˆì¸¡ ëŒ€ìƒ ì»¬ëŸ¼ ì„ íƒ", ["ìˆ˜í™•ìˆ˜", "í‰ê· ê³¼ì¤‘"] + growth_features)
    features = [col for col in df.columns if col not in ["ì¡°ì‚¬ì¼ì", "ìˆ˜í™•ìˆ˜", "í‰ê· ê³¼ì¤‘"] + growth_features]

    X = df[features]
    y = df[target_col]
    X = X.fillna(X.mean())
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    if model_choice == "RandomForest":
        model = RandomForestRegressor(random_state=42)
    elif model_choice == "GradientBoosting":
        model = GradientBoostingRegressor(random_state=42)
    elif model_choice == "XGBoost":
        model = XGBRegressor(random_state=42)
    elif model_choice == "LGBM":
        model = LGBMRegressor(random_state=42)
    elif model_choice == "GaussianNB":
        model = GaussianNB()

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    weeks2 = st.slider("í‰ê·  ê³„ì‚° ê¸°ê°„ (ì£¼ ë‹¨ìœ„) - ëª¨ë¸ìš©",
                       1, 7, st.session_state.weeks, key="weeks_slider_2", on_change=update_weeks_2)
    days = st.session_state.weeks * 7

    # --- í‰ê°€ì§€í‘œ ---
    st.subheader("ëª¨ë¸ í‰ê°€ ì§€í‘œ")
    st.write(f"MSE: {mean_squared_error(y_test, y_pred):.3f}")
    st.write(f"MAE: {mean_absolute_error(y_test, y_pred):.3f}")
    st.write(f"RÂ²: {r2_score(y_test, y_pred):.3f}")

    # ---------------------------
    # SHAP, Feature Importance ë ˆì´ì•„ì›ƒ ì¬ë°°ì¹˜ ë° ICE/PDP/ALE ì¶”ê°€
    # ---------------------------

    import math
    from sklearn.utils import check_array


    # ê°„ë‹¨í•œ ALE ê³„ì‚° í•¨ìˆ˜ (ìˆ˜ì¹˜í˜• feature ì „ìš©, ëª¨ë¸ì˜ predict ì‚¬ìš©)
    def compute_ale(model, X, feature, bins=10):
        """
        ê°„ë‹¨í•œ 1ì°¨ì› ALE ê·¼ì‚¬
        model: í•™ìŠµëœ ëª¨ë¸ (predict ë©”ì„œë“œ ì‚¬ìš©)
        X: DataFrame (ì›ë³¸ íŠ¹ì„± í–‰ë ¬)
        feature: feature ì´ë¦„(string)
        bins: bin ìˆ˜
        returns: bin_centers, ale_values
        """
        x = X[feature].values
        # remove nan rows for feature
        mask = ~np.isnan(x)
        x = x[mask]
        X_valid = X.loc[mask].reset_index(drop=True)
        percentiles = np.linspace(0, 100, bins + 1)
        cutpoints = np.percentile(x, percentiles)
        # ì¤‘ë³µ ì»·í¬ì¸íŠ¸ ì²˜ë¦¬: ìœ ë‹ˆí¬ë¡œ
        cutpoints = np.unique(cutpoints)
        if len(cutpoints) < 2:
            # ë³€ë™ì´ ê±°ì˜ ì—†ì„ ë•Œ
            return np.array([np.mean(x)]), np.array([0.0])

        # ê° êµ¬ê°„ë³„ í‰ê·  ê¸°ì—¬ ê³„ì‚°
        local_effects = []
        bin_centers = []
        for i in range(len(cutpoints) - 1):
            lo, hi = cutpoints[i], cutpoints[i + 1]
            # í•´ë‹¹ êµ¬ê°„ì— ì†í•˜ëŠ” ì¸ë±ìŠ¤
            in_bin = (X_valid[feature] >= lo) & (X_valid[feature] <= hi)
            if in_bin.sum() == 0:
                # í•´ë‹¹ êµ¬ê°„ì— ì ì´ ì—†ìœ¼ë©´ 0 ë„£ê¸°
                local_effects.append(0.0)
                bin_centers.append((lo + hi) / 2.0)
                continue
            X_lo = X_valid.copy()
            X_hi = X_valid.copy()
            # ì™¼ìª½ ê²½ê³„ê°’ìœ¼ë¡œ, ì˜¤ë¥¸ìª½ ê²½ê³„ê°’ìœ¼ë¡œ ë°”ê¿”ì„œ ì˜ˆì¸¡ ì°¨ì´ë¥¼ ë´„
            X_lo.loc[in_bin, feature] = lo
            X_hi.loc[in_bin, feature] = hi
            try:
                preds_hi = model.predict(X_hi)
                preds_lo = model.predict(X_lo)
            except Exception:
                # some models require numpy array
                preds_hi = model.predict(X_hi.values)
                preds_lo = model.predict(X_lo.values)
            diff = preds_hi - preds_lo
            # ì§€ì—­ í‰ê·  ê¸°ì—¬
            local_effect = diff[in_bin.values].mean() if in_bin.sum() > 0 else 0.0
            local_effects.append(local_effect)
            bin_centers.append((lo + hi) / 2.0)

        # ëˆ„ì í•©ìœ¼ë¡œ ALE ê³„ì‚° (baselineì„ 0ìœ¼ë¡œ ë§ì¶¤)
        ale = np.cumsum(local_effects)
        # í‰ê· ì„ 0 ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •
        ale = ale - ale.mean()
        return np.array(bin_centers), ale


    # --------------------- XAI: SHAP / FI / ICE / PDP / ALE + ìë™ ë¦¬í¬íŠ¸ ---------------------
    import shap
    from sklearn.inspection import PartialDependenceDisplay
    from sklearn.linear_model import LinearRegression

    st.subheader("SHAP / Feature Importance / ICE / PDP / ALE â€” ìë™ ë¦¬í¬íŠ¸ í¬í•¨")

    # ì•ˆì „í•˜ê²Œ í•„ìš”í•œ objects ì¤€ë¹„
    try:
        # model, X_train, X_test, features ë“±ì´ ì¡´ì¬í•œë‹¤ê³  ê°€ì •
        _ = model
        _ = X_train
        _ = X_test
    except Exception as e:
        st.error("ëª¨ë¸Â·ë°ì´í„°(ì˜ˆ: model, X_train, X_test ë“±)ê°€ ì¤€ë¹„ë˜ì–´ì•¼ XAIë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.stop()


    # ---------- ìœ í‹¸ í•¨ìˆ˜ë“¤ ----------
    def safe_inverse_transform(scaler, arr):
        try:
            return scaler.inverse_transform(arr)
        except Exception:
            return arr


    def find_top_contiguous_interval(x, y, top_frac=0.9, min_width=1):
        """
        x: 1d array (sorted)
        y: 1d array same length
        ëª©í‘œ: yê°€ ìµœëŒ€ì¸ ì—°ì† êµ¬ê°„ì„ ì°¾ì•„ (start, end, mean_y, max_y)
        ë°©ì•ˆ: yì˜ top percentile ì˜ì—­ì—ì„œ ê°€ì¥ ê¸´ ì—°ì† êµ¬ê°„ ì„ íƒ
        """
        import numpy as np
        thresh = np.quantile(y, top_frac)
        mask = y >= thresh
        # find contiguous true segments
        segments = []
        i = 0
        while i < len(mask):
            if mask[i]:
                j = i
                while j < len(mask) and mask[j]:
                    j += 1
                if (j - i) >= min_width:
                    segments.append((i, j - 1))
                i = j
            else:
                i += 1
        if not segments:
            # fallback: return argmax single point expanded by 1 on each side
            idx = int(np.argmax(y))
            left = max(0, idx - 1)
            right = min(len(x) - 1, idx + 1)
            return x[left], x[right], float(np.mean(y[left:right + 1])), float(np.max(y[left:right + 1]))
        # choose segment with largest mean y
        best = None
        best_score = -1e9
        for s, t in segments:
            score = float(np.mean(y[s:t + 1]))
            if score > best_score:
                best_score = score
                best = (s, t)
        s, t = best
        return x[s], x[t], float(np.mean(y[s:t + 1])), float(np.max(y[s:t + 1]))


    def summarize_pdp(model, X, feature, grid_resolution=50):
        """
        Compute PDP (average) and return summary info: best interval where PDP is high.
        """
        import numpy as np
        try:
            display = PartialDependenceDisplay.from_estimator(model, X, [feature], kind="average",
                                                              grid_resolution=grid_resolution)
            # sklearn returns axes data in different versions; try to extract x and y
            # Attempt 1: from display
            try:
                pdp_ax = display.axes_[0, 0]
                lines = pdp_ax.get_lines()
                if len(lines) > 0:
                    x = lines[0].get_xdata()
                    y = lines[0].get_ydata()
                else:
                    # fallback: use display.pd_results if present
                    x = display.pd_results[0][0]
                    y = display.pd_results[0][1]
            except Exception:
                # fallback: try attribute
                res = display.pd_results[0]
                x = res["values"]
                y = res["average"]
        except Exception:
            # fallback: manual grid eval
            col = X[feature]
            x = np.linspace(col.min(), col.max(), grid_resolution)
            y = []
            Xbase = X.copy()
            for val in x:
                Xtmp = Xbase.copy()
                Xtmp[feature] = val
                preds = model.predict(Xtmp)
                if preds.ndim == 2:
                    preds = preds.mean(axis=1)
                y.append(np.mean(preds))
            x = np.array(x)
            y = np.array(y)
        # find top contiguous interval (upper 10% area)
        start, end, mean_y, max_y = find_top_contiguous_interval(x, y, top_frac=0.9)
        return x, y, {"best_interval": (start, end), "mean_val": float(mean_y), "max_val": float(max_y)}


    def summarize_ice_linear_slope(model, X, feature, n_samples=50):
        """
        For ICE: sample up to n_samples rows, compute for each row the model prediction
        across a grid of feature values, and estimate slope via simple linear regression.
        Returns average slope (pred change per unit feature), and sign summary.
        """
        import numpy as np
        Xs = X.sample(n=min(n_samples, len(X)), random_state=42)
        xs = np.linspace(X[feature].min(), X[feature].max(), 30)
        slopes = []
        for _, row in Xs.iterrows():
            Xtmp = pd.DataFrame(np.tile(row.values, (len(xs), 1)), columns=X.columns)
            Xtmp[feature] = xs
            preds = model.predict(Xtmp)
            if preds.ndim == 2:
                # if multioutput, average
                preds = preds.mean(axis=1)
            # fit linear regression slope
            lr = LinearRegression()
            lr.fit(xs.reshape(-1, 1), preds)
            slopes.append(lr.coef_[0])
        slopes = np.array(slopes)
        return float(np.mean(slopes)), float(np.std(slopes)), len(slopes)


    def summarize_ale_intervals(bin_centers, ale_vals):
        """
        Analyze ALE array to find positive/negative intervals and steep changes (threshold by derivative)
        """
        import numpy as np
        bc = np.array(bin_centers)
        av = np.array(ale_vals)
        # derivative
        deriv = np.gradient(av, bc)
        # find peak positive regions
        pos_mask = av > 0
        neg_mask = av < 0
        # find steep points where absolute derivative > 1.5 * std(deriv)
        thr = 1.5 * (np.std(deriv) + 1e-9)
        steep_idx = np.where(np.abs(deriv) > thr)[0]

        # contiguous positive/negative ranges
        def contiguous_ranges(mask):
            ranges = []
            i = 0
            while i < len(mask):
                if mask[i]:
                    j = i
                    while j < len(mask) and mask[j]:
                        j += 1
                    ranges.append((i, j - 1))
                    i = j
                else:
                    i += 1
            return ranges

        pos_ranges = contiguous_ranges(pos_mask)
        neg_ranges = contiguous_ranges(neg_mask)
        # convert to value intervals
        pos_intervals = [(float(bc[s]), float(bc[t]), float(np.mean(av[s:t + 1]))) for s, t in pos_ranges]
        neg_intervals = [(float(bc[s]), float(bc[t]), float(np.mean(av[s:t + 1]))) for s, t in neg_ranges]
        steep_points = [(int(i), float(bc[i]), float(deriv[i])) for i in steep_idx]
        return {"pos_intervals": pos_intervals, "neg_intervals": neg_intervals, "steep_points": steep_points}


    # ---------- SHAP & FI (top row) ----------
    top_col1, top_col2 = st.columns([1, 1])

    with top_col1:
        st.markdown("### ğŸ” SHAP Summary")
        if model_choice == "GaussianNB":
            st.info("GaussianNB ëª¨ë¸ì€ SHAP ì‚¬ìš©ì´ ì œí•œì ì…ë‹ˆë‹¤.")
        else:
            try:
                # Compute or reuse SHAP
                try:
                    shap_values  # if existing
                except NameError:
                    explainer = shap.Explainer(model, X_train)
                    shap_values = explainer(X_test)
                # summary plot
                fig_shap, ax_shap = plt.subplots(figsize=(6, 4))
                shap.summary_plot(shap_values, X_test, show=False)
                st.pyplot(fig_shap)
                plt.close(fig_shap)
                # Summary table: mean(|shap|)
                shap_mean = np.abs(shap_values.values).mean(axis=0)
                shap_df = pd.DataFrame({"Feature": features, "Mean(|SHAP|)": shap_mean}).sort_values(by="Mean(|SHAP|)",
                                                                                                     ascending=False)
                st.dataframe(shap_df.head(12).round(6))
                # Text summary: top contributors
                top_feats = shap_df.head(5)
                text_lines = []
                total = shap_df["Mean(|SHAP|)"].sum()
                for i, row in top_feats.iterrows():
                    pct = 100.0 * row["Mean(|SHAP|)"] / total if total > 0 else 0.0
                    text_lines.append(f"{row['Feature']}: ì˜í–¥ë„ {pct:.1f}%")
                st.markdown("**ìƒìœ„ íŠ¹ì§•(Mean |SHAP| ê¸°ì¤€)**")
                for ln in text_lines:
                    st.write("â€¢", ln)
            except Exception as e:
                st.error(f"SHAP ê³„ì‚°/ì‹œê°í™” ì˜¤ë¥˜: {e}")

    with top_col2:
        st.markdown("### ğŸ“Š Feature Importance (Model-based)")
        try:
            if hasattr(model, "feature_importances_"):
                importances = model.feature_importances_
                fi_df = pd.DataFrame({"Feature": features, "Importance": importances}).sort_values(by="Importance",
                                                                                                   ascending=False)
            else:
                # fallback: permutation importance would be better; here zero-fill
                fi_df = pd.DataFrame({"Feature": features, "Importance": np.zeros(len(features))}).sort_values(
                    by="Importance", ascending=False)
                st.warning("ì„ íƒ ëª¨ë¸ì— feature_importances_ ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤. Permutation importance ê¶Œì¥ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            # plot
            fig_fi, ax_fi = plt.subplots(figsize=(6, 4))
            ax_fi.barh(fi_df["Feature"], fi_df["Importance"])
            ax_fi.invert_yaxis()
            ax_fi.set_title("Feature Importance")
            st.pyplot(fig_fi)
            plt.close(fig_fi)
            # text summary
            st.markdown("**Feature Importance ìš”ì•½**")
            top = fi_df.head(5)
            tot = fi_df["Importance"].sum()
            for _, r in top.iterrows():
                pct = 100.0 * r["Importance"] / tot if tot > 0 else 0.0
                st.write(f"â€¢ {r['Feature']}: {pct:.1f}%")
        except Exception as e:
            st.error(f"Feature Importance ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # ---------- SHAP ìƒì„¸: íŠ¹ì • ìƒ˜í”Œ ì˜í–¥ (ì˜ˆ: CO2ê°€ +2kg ê¸°ì—¬ ê°™ì€ ë¬¸ì¥) ----------
    st.markdown("### ğŸ” SHAP ìƒ˜í”Œë³„ ìƒì„¸ í•´ì„")
    sample_idx = st.number_input("ìƒ˜í”Œ ì¸ë±ìŠ¤ (X_test ê¸°ì¤€)", min_value=0, max_value=max(0, len(X_test) - 1), value=0, step=1)
    if model_choice != "GaussianNB":
        try:
            # get one sample
            xi = X_test.reset_index(drop=True).iloc[sample_idx:sample_idx + 1]
            # shap values for that sample
            shp_s = shap_values[sample_idx].values  # shape (n_features,)
            shp_df = pd.DataFrame({"Feature": features, "SHAP": shp_s}).sort_values(by="SHAP", key=lambda s: np.abs(s),
                                                                                    ascending=False)
            st.dataframe(shp_df.head(20).round(4))
            # textual interpretation: top positive / negative contributors
            pos = shp_df[shp_df["SHAP"] > 0].head(5)
            neg = shp_df[shp_df["SHAP"] < 0].head(5)
            if not pos.empty:
                st.write("ìƒìœ„ ì–‘(+) ê¸°ì—¬ ë³€ìˆ˜:")
                for _, r in pos.iterrows():
                    st.write(f"â€¢ {r['Feature']}: ì˜ˆì¸¡ ì¦ê°€ì— ê¸°ì—¬ +{r['SHAP']:.3f}")
            if not neg.empty:
                st.write("ìƒìœ„ ìŒ(-) ê¸°ì—¬ ë³€ìˆ˜:")
                for _, r in neg.iterrows():
                    st.write(f"â€¢ {r['Feature']}: ì˜ˆì¸¡ ê°ì†Œì— ê¸°ì—¬ {r['SHAP']:.3f}")
            # Example style sentence (ìë™ìƒì„±)
            if "CO2" in shp_df["Feature"].values or "COâ‚‚" in shp_df["Feature"].values:
                # find CO2 row if exists
                co2_rows = shp_df[shp_df["Feature"].str.contains("CO2|COâ‚‚", regex=True)]
                if not co2_rows.empty:
                    r = co2_rows.iloc[0]
                    sign = "+" if r["SHAP"] > 0 else "-"
                    st.info(f"ì˜ˆì‹œ í•´ì„: íŠ¹ì • ìƒ˜í”Œì—ì„œ COâ‚‚ ë†ë„ëŠ” ìˆ˜í™•ëŸ‰ ì˜ˆì¸¡ì— {sign}{abs(r['SHAP']):.3f}ë§Œí¼ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"SHAP ìƒ˜í”Œ ë¶„ì„ ì˜¤ë¥˜: {e}")
    else:
        st.info("GaussianNB ëª¨ë¸ì€ SHAP ìƒì„¸ ë¶„ì„ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # ---------- í•˜ë‹¨: ICE / PDP / ALE (ê·¸ë˜í”„ + ìë™ ë¦¬í¬íŠ¸) ----------
    st.subheader("ICE / PDP / ALE â€” ê·¸ë˜í”„ + ìµœì  êµ¬ê°„ ë¦¬í¬íŠ¸")

    ice_feature = st.selectbox("ë¶„ì„í•  Feature ì„ íƒ (ICE/PDP/ALE)", features, key="xai_feature")
    n_samples = st.slider("ICE ìƒ˜í”Œ ìˆ˜", 1, max(1, len(X_test)), value=min(50, len(X_test)), key="ice_samples")
    ale_bins = st.slider("ALE bins ìˆ˜", 4, 30, 10)

    col_ice, col_pdp, col_ale = st.columns(3)

    # ICE
    with col_ice:
        st.markdown("**ICE Plot & ë¯¼ê°ë„(ë‹¨ìœ„ë‹¹ ë³€í™”ëŸ‰)**")
        try:
            fig_ice, ax_ice = plt.subplots(figsize=(5, 3))
            try:
                PartialDependenceDisplay.from_estimator(model,
                                                        X_test.sample(n=min(n_samples, len(X_test)), random_state=42),
                                                        features=[ice_feature], kind="individual", ax=ax_ice,
                                                        line_kw={"alpha": 0.3})
            except Exception:
                # fallback simple: draw for sampled rows
                Xs = X_test.sample(n=min(n_samples, len(X_test)), random_state=42)
                xs = np.linspace(X_test[ice_feature].min(), X_test[ice_feature].max(), 50)
                for _, row in Xs.iterrows():
                    Xtmp = pd.DataFrame(np.tile(row.values, (len(xs), 1)), columns=X_test.columns)
                    Xtmp[ice_feature] = xs
                    preds = model.predict(Xtmp)
                    if preds.ndim == 2:
                        preds = preds.mean(axis=1)
                    ax_ice.plot(xs, preds, alpha=0.2)
            ax_ice.set_title(f"ICE: {ice_feature}")
            ax_ice.set_xlabel(ice_feature)
            ax_ice.set_ylabel("Predicted")
            st.pyplot(fig_ice)
            plt.close(fig_ice)
            # sensitivity summary (slope)
            mean_slope, std_slope, cnt = summarize_ice_linear_slope(model, X_test, ice_feature,
                                                                    n_samples=min(n_samples, len(X_test)))
            st.write(f"ìƒ˜í”Œ {cnt}ê°œ í‰ê·  ê¸°ìš¸ê¸°(ë‹¨ìœ„ {ice_feature} ë‹¹ ì˜ˆì¸¡ ë³€í™”): {mean_slope:.4f} Â± {std_slope:.4f}")
            if mean_slope > 0:
                st.info(f"í•´ì„: {ice_feature}ê°€ ì¦ê°€í•  ë•Œ í‰ê· ì ìœ¼ë¡œ ì˜ˆì¸¡(ìˆ˜í™•ëŸ‰ ë“±)ë„ ì¦ê°€í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.")
            elif mean_slope < 0:
                st.info(f"í•´ì„: {ice_feature}ê°€ ì¦ê°€í•  ë•Œ í‰ê· ì ìœ¼ë¡œ ì˜ˆì¸¡ì´ ê°ì†Œí•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.")
            else:
                st.info("í•´ì„: í‰ê· ì ì¸ ë¯¼ê°ë„ê°€ ê±°ì˜ 0ì…ë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ICE ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # PDP
    with col_pdp:
        st.markdown("**PDP (Average) & ìµœì  êµ¬ê°„**")
        try:
            xvals, yvals, pdp_summary = summarize_pdp(model, X_test, ice_feature, grid_resolution=50)
            fig_pdp, ax_pdp = plt.subplots(figsize=(5, 3))
            ax_pdp.plot(xvals, yvals, color="red", lw=2)
            ax_pdp.set_title(f"PDP: {ice_feature}")
            ax_pdp.set_xlabel(ice_feature)
            ax_pdp.set_ylabel("Predicted")
            st.pyplot(fig_pdp)
            plt.close(fig_pdp)
            s, e, meanv, maxv = pdp_summary["best_interval"][0], pdp_summary["best_interval"][1], pdp_summary[
                "mean_val"], pdp_summary["max_val"]
            # NOTE: summarize_pdp returns dict with best_interval tuple as value, but above we returned differently; handle both
            try:
                start, end = pdp_summary["best_interval"]
            except Exception:
                start, end = s, e
            st.write(f"ìµœì (ì˜ˆì¸¡ì´ í°) êµ¬ê°„: {start:.3f} ~ {end:.3f}")
            st.write(f"êµ¬ê°„ í‰ê·  ì˜ˆì¸¡ê°’: {pdp_summary['mean_val']:.3f}, êµ¬ê°„ ìµœëŒ€ê°’: {pdp_summary['max_val']:.3f}")
            # agricultural interpretation template
            st.markdown("**ë†ì—…ì  í•´ì„(ì˜ˆì‹œ)**")
            st.write(f"â€¢ ë§Œì•½ {ice_feature}ê°€ {start:.1f}â€“{end:.1f} êµ¬ê°„ì— ìì£¼ ë¨¸ë¬¸ë‹¤ë©´ ëª¨ë¸ì€ ì´ êµ¬ê°„ì„ ë¹„êµì  ìš°í˜¸ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"PDP ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # ALE
    with col_ale:
        st.markdown("**ALE (ê·¼ì‚¬) & ì„ê³„êµ¬ê°„ íƒì§€**")
        try:
            bin_centers, ale_vals = compute_ale(model, X_test.reset_index(drop=True), ice_feature, bins=ale_bins)
            fig_ale, ax_ale = plt.subplots(figsize=(5, 3))
            if len(bin_centers) > 1:
                ax_ale.plot(bin_centers, ale_vals, marker="o", linestyle="-")
            else:
                ax_ale.hlines(ale_vals[0], bin_centers[0] - 0.5, bin_centers[0] + 0.5)
            ax_ale.set_title(f"ALE (approx): {ice_feature}")
            ax_ale.set_xlabel(ice_feature)
            ax_ale.set_ylabel("ALE")
            st.pyplot(fig_ale)
            plt.close(fig_ale)
            # summarize ALE
            ale_summary = summarize_ale_intervals(bin_centers, ale_vals)
            if ale_summary["pos_intervals"]:
                st.write("ëª¨ë¸ì´ ìš°í˜¸ì ìœ¼ë¡œ ë³´ëŠ” êµ¬ê°„(ì–‘ì˜ ALE):")
                for a, b, mv in ale_summary["pos_intervals"]:
                    st.write(f"â€¢ {a:.2f} ~ {b:.2f} (í‰ê·  ALE: {mv:.3f})")
            if ale_summary["neg_intervals"]:
                st.write("ëª¨ë¸ì´ ë¶ˆë¦¬í•˜ê²Œ ë³´ëŠ” êµ¬ê°„(ìŒì˜ ALE):")
                for a, b, mv in ale_summary["neg_intervals"]:
                    st.write(f"â€¢ {a:.2f} ~ {b:.2f} (í‰ê·  ALE: {mv:.3f})")
            if ale_summary["steep_points"]:
                st.write("ALEì—ì„œ ê¸‰ê²©íˆ ë³€í™”í•˜ëŠ” ì (ì„ê³„ì ) ì˜ˆì‹œ:")
                for idx, val, deriv in ale_summary["steep_points"][:5]:
                    st.write(f"â€¢ idx {idx}, {ice_feature}â‰ˆ{val:.2f}, ê¸°ìš¸ê¸°â‰ˆ{deriv:.3f}")
            # example agricultural interpretation template
            st.markdown("**ë†ì—…ì (ìƒë¦¬í•™ì ) í•´ì„ ì˜ˆì‹œ í…œí”Œë¦¿**")
            st.write("â€¢ ëª¨ë¸ì´ íŠ¹ì • ì˜¨ë„êµ¬ê°„ì„ ìš°í˜¸ì ìœ¼ë¡œ í‰ê°€í•œë‹¤ë©´(ì˜ˆ: 20~21â„ƒ), í•´ë‹¹ êµ¬ê°„ì—ì„œ ê´‘í•©ì„±Â·ê°œí™”Â·ì°©ê³¼ê°€ ìœ ë¦¬í•  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
            st.write("â€¢ ë°˜ëŒ€ë¡œ íŠ¹ì • êµ¬ê°„ì—ì„œ ALEê°€ ê¸‰ê²©íˆ ê°ì†Œí•˜ë©´(ì„ê³„ì˜¨ë„ ì¡´ì¬), ê·¸ ì§€ì ì„ ì•ŒëŒìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê´€ë¦¬/í™˜ê¸°/ì°¨ê´‘ ë“±ì˜ ì œì–´ì „ëµì„ ê³ ë ¤í•˜ì„¸ìš”.")
        except Exception as e:
            st.error(f"ALE ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    # ---------------------------
    # 2D ALE (interaction) + ALE with bootstrap CI
    # ---------------------------
    import numpy as np
    import matplotlib.pyplot as plt


    def _safe_predict(model, X_df):
        """model.predictê°€ DataFrameì„ ë°”ë¡œ ë°›ì§€ ëª»í•  ê²½ìš° ëŒ€ë¹„"""
        try:
            return model.predict(X_df)
        except Exception:
            return model.predict(X_df.values)


    def compute_2d_ale(model, X, feat_x, feat_y, grid=10, min_count_in_cell=1):
        """
        2D ALE (second-order interaction) approximation.
        Returns:
            x_centers, y_centers, ale2d (shape (len(x_centers), len(y_centers)))
        Method (simplified Apley & Zhu):
          - Partition feat_x and feat_y into bins (quantile-based)
          - For each cell (i,j) compute average second-order finite diff:
              delta = f(x_hi,y_hi) - f(x_lo,y_hi) - f(x_hi,y_lo) + f(x_lo,y_lo)
          - raw_interaction[i,j] = mean(delta over samples in cell)
          - cumulative (double sum) over x then y to get ALE surface
          - center ALE to have mean 0
        """
        X = X.copy().reset_index(drop=True)
        xv = X[feat_x].values
        yv = X[feat_y].values

        # edges by quantile to respect data density
        x_edges = np.unique(np.percentile(xv, np.linspace(0, 100, grid + 1)))
        y_edges = np.unique(np.percentile(yv, np.linspace(0, 100, grid + 1)))

        # if too few unique edges -> fallback to unique centers
        if len(x_edges) < 2:
            return np.array([np.mean(xv)]), np.array([np.mean(yv)]), np.array([[0.0]])
        if len(y_edges) < 2:
            return np.array([np.mean(xv)]), np.array([np.mean(yv)]), np.array([[0.0]])

        nx = len(x_edges) - 1
        ny = len(y_edges) - 1

        raw = np.zeros((nx, ny))
        counts = np.zeros((nx, ny), dtype=int)

        # Precompute bin membership
        x_bin_idx = np.digitize(xv, x_edges, right=False) - 1  # 0..nx-1
        y_bin_idx = np.digitize(yv, y_edges, right=False) - 1  # 0..ny-1

        # clamp indices (edge cases)
        x_bin_idx = np.clip(x_bin_idx, 0, nx - 1)
        y_bin_idx = np.clip(y_bin_idx, 0, ny - 1)

        # For each cell compute the average second-order diff
        for i in range(nx):
            x_lo, x_hi = x_edges[i], x_edges[i + 1]
            for j in range(ny):
                y_lo, y_hi = y_edges[j], y_edges[j + 1]
                # mask of samples that fall in this cell
                mask = (x_bin_idx == i) & (y_bin_idx == j)
                idxs = np.where(mask)[0]
                counts[i, j] = len(idxs)
                if len(idxs) < min_count_in_cell:
                    raw[i, j] = 0.0
                    continue

                # build 4 matrices for modified rows (only for rows in cell)
                X_ll = X.loc[idxs].copy()  # x_lo, y_lo
                X_lh = X.loc[idxs].copy()  # x_lo, y_hi
                X_hl = X.loc[idxs].copy()  # x_hi, y_lo
                X_hh = X.loc[idxs].copy()  # x_hi, y_hi

                X_ll[feat_x] = x_lo;
                X_ll[feat_y] = y_lo
                X_lh[feat_x] = x_lo;
                X_lh[feat_y] = y_hi
                X_hl[feat_x] = x_hi;
                X_hl[feat_y] = y_lo
                X_hh[feat_x] = x_hi;
                X_hh[feat_y] = y_hi

                p_hh = _safe_predict(model, X_hh)
                p_hl = _safe_predict(model, X_hl)
                p_lh = _safe_predict(model, X_lh)
                p_ll = _safe_predict(model, X_ll)

                # second-order finite difference per sample
                delta = p_hh - p_hl - p_lh + p_ll
                raw[i, j] = np.mean(delta)

        # double cumulative sum to obtain ALE surface (integration)
        cum_x = np.cumsum(raw, axis=0)  # cumulative over x for each y
        cum_xy = np.cumsum(cum_x, axis=1)  # then cumulative over y

        # The above orientation gives shape (nx, ny). Depending on plotting, may transpose later.
        ale2d = cum_xy

        # Centering: subtract mean (only over cells with counts>0)
        valid = counts > 0
        if valid.any():
            mean_val = np.mean(ale2d[valid])
        else:
            mean_val = 0.0
        ale2d = ale2d - mean_val

        # bin centers for plotting
        x_centers = (x_edges[:-1] + x_edges[1:]) / 2.0
        y_centers = (y_edges[:-1] + y_edges[1:]) / 2.0

        return x_centers, y_centers, ale2d


    def compute_ale_with_bootstrap_ci(model, X, feature, bins=10, B=50, ci=(2.5, 97.5), random_state=42):
        """
        Compute 1D ALE and bootstrap confidence interval.
        Returns:
          bin_centers, ale_mean, ale_lower, ale_upper, ale_all (B x n_bins array)
        """
        rng = np.random.RandomState(random_state)
        ale_list = []
        # If dataset small, reduce B
        for b in range(B):
            # bootstrap sample of rows (with replacement)
            idxs = rng.randint(0, len(X), size=len(X))
            Xb = X.iloc[idxs].reset_index(drop=True)
            centers, ale_vals = compute_ale(model, Xb, feature, bins=bins)
            # ensure consistent length: if compute_ale returns single-point for degenerate, pad/reshape
            ale_list.append(ale_vals)

        # pad to same length if necessary (rare if bins same)
        lengths = [len(a) for a in ale_list]
        maxlen = max(lengths)
        arr = np.zeros((B, maxlen))
        arr[:] = np.nan
        for i, a in enumerate(ale_list):
            arr[i, :len(a)] = a

        # compute mean and percentile CI ignoring nan
        ale_mean = np.nanmean(arr, axis=0)
        ale_lower = np.nanpercentile(arr, ci[0], axis=0)
        ale_upper = np.nanpercentile(arr, ci[1], axis=0)

        # use centers from last run or recompute on original X for a reliable grid
        centers, _ = compute_ale(model, X, feature, bins=bins)
        return centers, ale_mean, ale_lower, ale_upper, arr


    # ---------------------------
    # Streamlit UI additions to call 2D ALE and bootstrap CI
    # ---------------------------
    st.markdown("---")
    st.subheader("ì¶”ê°€: 2ì°¨ì› ALE (êµí˜¸ì‘ìš©) ë° ALE ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„")

    # ==========================================================
    # (1) 2ì°¨ì› ALE (êµí˜¸ì‘ìš©) + ìë™ í•´ì„ ë¦¬í¬íŠ¸
    # ==========================================================
    with st.expander("2ì°¨ì› ALE (feat A Ã— feat B) ê³„ì‚° / ì‹œê°í™”"):
        col_left, col_right = st.columns([1, 1])  # ì™¼ìª½ UI, ì˜¤ë¥¸ìª½ ê·¸ë˜í”„

        with col_left:
            feat_x_2d = st.selectbox("X ì¶• (Feature A)", features, index=0, key="feat_x_2d")
            feat_y_2d = st.selectbox(
                "Y ì¶• (Feature B)", [f for f in features if f != feat_x_2d], index=0, key="feat_y_2d"
            )
            grid_size = st.slider("ê·¸ë¦¬ë“œ í¬ê¸° (ê° ì¶• bin ìˆ˜)", 4, 20, 8, key="ale2d_grid")
            min_count = st.number_input(
                "ì…€ ë‹¹ ìµœì†Œ ìƒ˜í”Œ ìˆ˜ (ë…¸ì´ì¦ˆ ë°©ì§€)", min_value=1, max_value=100, value=1, step=1
            )
            run_ale2d = st.button("2D ALE ê³„ì‚°/ì‹œê°í™”", key="run_ale2d")

        with col_right:
            if run_ale2d:
                try:
                    with st.spinner("2D ALE ê³„ì‚° ì¤‘..."):
                        x_centers, y_centers, ale2d = compute_2d_ale(
                            model, X_test.reset_index(drop=True),
                            feat_x_2d, feat_y_2d,
                            grid=grid_size, min_count_in_cell=min_count
                        )

                    # ----------- ì‹œê°í™” -----------
                    fig2d, ax2d = plt.subplots(figsize=(6, 4))
                    XX, YY = np.meshgrid(x_centers, y_centers, indexing='xy')

                    cs = ax2d.contourf(XX, YY, ale2d.T, cmap='RdBu_r', levels=20)
                    ax2d.set_xlabel(feat_x_2d)
                    ax2d.set_ylabel(feat_y_2d)
                    ax2d.set_title(f"2D ALE interaction: {feat_x_2d} Ã— {feat_y_2d}")
                    fig2d.colorbar(cs, ax=ax2d, label="ALE (interaction)")
                    st.pyplot(fig2d)
                    plt.close(fig2d)

                    # ========================================================
                    # (ì¶”ê°€) ìë™ í•´ì„ ë¦¬í¬íŠ¸ ìƒì„±
                    # ========================================================
                    max_ale = np.max(ale2d)
                    min_ale = np.min(ale2d)

                    max_loc = np.unravel_index(np.argmax(ale2d), ale2d.shape)
                    min_loc = np.unravel_index(np.argmin(ale2d), ale2d.shape)

                    x_max = x_centers[max_loc[0]]
                    y_max = y_centers[max_loc[1]]
                    x_min = x_centers[min_loc[0]]
                    y_min = y_centers[min_loc[1]]

                    # ë¦¬í¬íŠ¸ ìƒì„±
                    st.subheader("ğŸ“˜ 2D ALE ìë™ í•´ì„ ë¦¬í¬íŠ¸")

                    report = f"""
    ### ğŸ” **êµí˜¸ì‘ìš© ë¶„ì„ ê°œìš”**
    íŠ¹ì„± **{feat_x_2d}** ì™€ **{feat_y_2d}** ì˜ ì¡°í•©ì´ ëª¨ë¸ ì˜ˆì¸¡(ìˆ˜í™•ëŸ‰)ì— ë¯¸ì¹˜ëŠ”  
    **ìˆœìˆ˜í•œ ìƒí˜¸ì‘ìš© íš¨ê³¼(Interaction Effect)**ë¥¼ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.  
    ì¦‰, ë‘ ë³€ìˆ˜ê°€ í•¨ê»˜ ë³€í™”í•  ë•Œ ë‹¨ë… íš¨ê³¼ë¡œ ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ì¶”ê°€ì ì¸ ì˜í–¥ë§Œì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

    ---

    ### ğŸ”º **1) ìˆ˜í™•ëŸ‰ ì¦ê°€ì— ê°€ì¥ í¬ê²Œ ê¸°ì—¬í•˜ëŠ” ì¡°í•© (ì–‘ì˜ ìƒí˜¸ì‘ìš©)**
    - ALE ìµœëŒ€ê°’: **{max_ale:.3f}**
    - ë°œìƒ êµ¬ê°„:  
      - **{feat_x_2d}: {x_max:.2f}**  
      - **{feat_y_2d}: {y_max:.2f}**

    â¡ï¸ ì´ êµ¬ê°„ì—ì„œëŠ” ë‘ ë³€ìˆ˜ê°€ ê²°í•©í•  ë•Œ **ìˆ˜í™•ëŸ‰ì„ ê°•í•˜ê²Œ ì¦ê°€ì‹œí‚¤ëŠ” ìƒìœ¡ ì¡°ê±´**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

    ---

    ### ğŸ”» **2) ìˆ˜í™•ëŸ‰ ê°ì†Œì— ê°€ì¥ í¬ê²Œ ê¸°ì—¬í•˜ëŠ” ì¡°í•© (ìŒì˜ ìƒí˜¸ì‘ìš©)**
    - ALE ìµœì†Œê°’: **{min_ale:.3f}**
    - ë°œìƒ êµ¬ê°„:  
      - **{feat_x_2d}: {x_min:.2f}**  
      - **{feat_y_2d}: {y_min:.2f}**

    â¡ï¸ ì´ ì¡°í•©ì€ ë‘ ë³€ìˆ˜ê°€ ë™ì‹œì— ì¡´ì¬í•  ë•Œ  
    ë‹¨ë… íš¨ê³¼ë³´ë‹¤ **ë” í° ìˆ˜í™•ëŸ‰ ê°ì†Œ íš¨ê³¼**ë¥¼ ì¼ìœ¼í‚¨ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

    ---

    ### ğŸ§­ **3) ìƒí˜¸ì‘ìš© íŒ¨í„´ ìš”ì•½**
    - ğŸ”´ **ì–‘ì˜ ALE(ë¹¨ê°„ìƒ‰)** â†’ ìˆ˜í™•ëŸ‰ ì¦ê°€ ì¡°í•©  
    - ğŸ”µ **ìŒì˜ ALE(íŒŒë€ìƒ‰)** â†’ ìˆ˜í™•ëŸ‰ ê°ì†Œ ì¡°í•©  
    - âšª **0 ê·¼ì²˜(í°ìƒ‰)** â†’ ìƒí˜¸ì‘ìš©ì´ ê±°ì˜ ì—†ìŒ  

    ë‘ ë³€ìˆ˜ê°€ ì„œë¡œ ì˜í–¥ì„ ì¦í­í•˜ê±°ë‚˜ ìƒì‡„í•˜ëŠ” **ë¹„ì„ í˜• êµ¬ì¡°**ê°€ ì¡´ì¬í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

    ---

    ### ğŸŒ± **4) ë†ì—…ì  í•´ì„ (í† ë§ˆí†  ê¸°ì¤€)**
    - ì–‘ì˜ ì¡°í•© êµ¬ê°„ì€ **ìƒìœ¡ì— ìœ ë¦¬í•˜ê±°ë‚˜ ê´‘Â·ì˜¨ë„ í™˜ê²½ì´ ì ì ˆí•˜ê²Œ ë§ì•„ ë–¨ì–´ì§€ëŠ” ì¡°ê±´**  
    - ìŒì˜ ì¡°í•© êµ¬ê°„ì€ **ì—´Â·ê´‘ ìŠ¤íŠ¸ë ˆìŠ¤, ê³¼ìŠµ, ë¹„íš¨ìœ¨ ê´‘í•©ì„± ë°œìƒ ê°€ëŠ¥**  
    - íŠ¹ì • ì˜¨ë„ì—ì„œ ì¼ì‚¬ëŸ‰ì´ ì¦ê°€í•  ë•Œ ë˜ëŠ” íŠ¹ì • ì¼ì‚¬ëŸ‰ì—ì„œ ì˜¨ë„ê°€ ìƒìŠ¹í•  ë•Œ  
      **ìˆ˜í™•ëŸ‰ì´ ê¸‰ê²©íˆ ì¦ê°€/ê°ì†Œí•˜ëŠ” êµ¬ê°„ì´ ì¡´ì¬í•¨ì„ ëª¨ë¸ì´ í•™ìŠµí–ˆë‹¤ëŠ” ì˜ë¯¸**  

    ---

    ### ğŸ“Œ **5) ì˜ì‚¬ê²°ì • í™œìš©**
    ë³¸ ë¶„ì„ì€ ë‹¤ìŒ ì˜ì‚¬ê²°ì •ì— ìœ ìš©í•©ë‹ˆë‹¤:
    - ìµœì  ìƒìœ¡ê¸° ì¡°ì„±(ì˜¨ë„Â·ì¼ì‚¬ëŸ‰ ì¡°í•© ì„¤ì •)
    - ìŠ¤íŠ¸ë ˆìŠ¤ í™˜ê²½ ì¡°ê¸° ì˜ˆì¸¡
    - í™˜ê²½ ì œì–´ ì¥ë¹„(ìŠ¤í¬ë¦°Â·í™˜ê¸°Â·ë‚œë°© ë“±) ì„¤ì • ê¸°ì¤€ ìˆ˜ë¦½
    - ì‘ê¸°ë³„(ì´ˆê¸°/ì¤‘ê¸°/í›„ê¸°) ìµœì  ì˜¨Â·ê´‘ ì¡°ê±´ ë„ì¶œ

    ---

    ### ğŸ“„ **ìš”ì•½**
    2D ALE ë¶„ì„ì„ í†µí•´  
    **â€œ{feat_x_2d}ì™€(ê³¼) {feat_y_2d}ì˜ ì¡°í•©ì´ ë‹¨ë… ë³€í™”ë³´ë‹¤ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ìˆ˜í™•ëŸ‰ì— ë¹„ì„ í˜•ì ì¸ ì˜í–¥ì„ ì£¼ëŠ”ì§€â€**  
    ì§ì ‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    ì´ ë¦¬í¬íŠ¸ëŠ” ëª¨ë¸ì´ ì‹ë¬¼ ìƒìœ¡ì˜ ìƒí˜¸ì‘ìš© íŒ¨í„´ì„ ì–´ë–»ê²Œ í•™ìŠµí–ˆëŠ”ì§€  
    ì •ëŸ‰ì Â·ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ëŠ” ìë™ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.
    """

                    st.markdown(report)

                except Exception as e:
                    st.error(f"2D ALE ê³„ì‚°/ì‹œê°í™” ì˜¤ë¥˜: {e}")

    # ==========================================================
    # (2) 1D ALE ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„ (CI)  - ì•ˆì „í•œ ê¸¸ì´/NaN ì²˜ë¦¬ ì¶”ê°€
    # ==========================================================
    with st.expander("1D ALE ë¶€íŠ¸ìŠ¤íŠ¸ë© ì‹ ë¢°êµ¬ê°„ (CI)"):
        col_left, col_right = st.columns([1, 1])

        with col_left:
            ci_feature = st.selectbox("ALE CI ëŒ€ìƒ Feature", features, index=0, key="ci_feature")
            B = st.slider("ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°˜ë³µ ìˆ˜ B", 10, 300, 50, step=10, key="ale_boot_B")
            alpha = st.slider("ì‹ ë¢°êµ¬ê°„(%) - ìƒí•œ/í•˜í•œ í¼ì„¼íƒ€ì¼ ì¤‘ì‹¬ê°’", 80, 99, 95, key="ale_boot_alpha")
            lower_pct = (100 - alpha) / 2.0
            upper_pct = 100 - lower_pct
            run_ale_ci = st.button("ALE + ë¶€íŠ¸ìŠ¤íŠ¸ë© CI ê³„ì‚°", key="run_ale_ci")

        with col_right:
            if run_ale_ci:
                try:
                    with st.spinner("ALE ë¶€íŠ¸ìŠ¤íŠ¸ë© ê³„ì‚° ì¤‘..."):
                        centers, mean_ale, lower_ale, upper_ale, all_vals = compute_ale_with_bootstrap_ci(
                            model, X_test.reset_index(drop=True), ci_feature,
                            bins=ale_bins, B=B, ci=(lower_pct, upper_pct)
                        )

                    # -----------------------------------------------
                    # 1) ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬
                    # -----------------------------------------------
                    if centers is None or len(centers) == 0:
                        st.warning("ALE ê³„ì‚° ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„° ë˜ëŠ” bins ê°’ì„ í™•ì¸í•˜ì„¸ìš”.")
                    else:
                        # ê¸¸ì´ ì¼ê´€ì„± í™•ë³´: ìµœì†Œ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìë¥¸ë‹¤
                        len_list = [len(arr) for arr in [centers, mean_ale, lower_ale, upper_ale] if arr is not None]
                        if len(len_list) == 0:
                            st.warning("ALE ê²°ê³¼ ë°°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                        else:
                            min_len = min(len_list)

                            # ì˜ë¼ë‚´ê¸°(í˜¹ì€ ë³€í™˜)
                            centers_s = np.array(centers)[:min_len]
                            mean_s = np.array(mean_ale)[:min_len]
                            lower_s = np.array(lower_ale)[:min_len]
                            upper_s = np.array(upper_ale)[:min_len]

                            # -----------------------------------------------
                            # 2) NaN ìˆëŠ” ì¸ë±ìŠ¤ ì œê±°
                            # -----------------------------------------------
                            valid_mask = (~np.isnan(centers_s)) & (~np.isnan(mean_s)) & (~np.isnan(lower_s)) & (
                                ~np.isnan(upper_s))
                            valid_idx = np.where(valid_mask)[0]

                            if valid_idx.size < 2:
                                st.warning("ìœ íš¨í•œ ALE í¬ì¸íŠ¸ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ (2 ë¯¸ë§Œ). ë” ë§ì€ ë°ì´í„° ë˜ëŠ” bin ìˆ˜ë¥¼ ë³€ê²½í•˜ì„¸ìš”.")
                            else:
                                # ì•ˆì „í•œ plot ë°ì´í„°
                                centers_plot = centers_s[valid_idx]
                                mean_plot = mean_s[valid_idx]
                                lower_plot = lower_s[valid_idx]
                                upper_plot = upper_s[valid_idx]

                                # -----------------------------------------------
                                # 3) all_vals ì •ë¦¬: (B x k) -> (B x valid_points)
                                # -----------------------------------------------
                                all_vals_arr = np.array(all_vals)  # ë‹¤ì°¨ì›ì¼ ìˆ˜ ìˆìŒ
                                # ë³´ì¥: all_vals_arr.shape == (B_eff, k_eff) í˜¹ì€ (B_eff,) ë“±
                                if all_vals_arr.ndim == 1:
                                    # ë§Œì•½ (B,)ì¸ ê²½ìš° B=1 í˜¹ì€ ì˜ëª»ëœ shape; reshape ì‹œë„
                                    all_vals_arr = all_vals_arr.reshape((all_vals_arr.shape[0], 1))
                                B_eff = all_vals_arr.shape[0]
                                k_eff = all_vals_arr.shape[1]

                                # pad or truncate to min_len
                                if k_eff < min_len:
                                    pad_width = min_len - k_eff
                                    all_vals_arr = np.concatenate([all_vals_arr, np.full((B_eff, pad_width), np.nan)],
                                                                  axis=1)
                                # truncate then select valid indices
                                all_vals_trunc = all_vals_arr[:, :min_len][:, valid_idx]  # shape (B_eff, valid_points)
                                std_boot = np.nanstd(all_vals_trunc, axis=0)

                                # -----------------------------------------------
                                # 4) ê·¸ë¦¬ê¸° (ê¸¸ì´/NaN ë¬¸ì œ í•´ê²° í›„)
                                # -----------------------------------------------
                                fig_ci, ax_ci = plt.subplots(figsize=(6, 3))
                                ax_ci.plot(centers_plot, mean_plot, marker='o', label='ALE mean')
                                ax_ci.fill_between(centers_plot, lower_plot, upper_plot, alpha=0.3,
                                                   label=f"{alpha}% CI")
                                ax_ci.set_title(f"ALE with {alpha}% bootstrap CI: {ci_feature}")
                                ax_ci.set_xlabel(ci_feature)
                                ax_ci.set_ylabel("ALE")
                                ax_ci.legend()
                                st.pyplot(fig_ci)
                                plt.close(fig_ci)

                                # -----------------------------------------------
                                # 5) ìš”ì•½í‘œ ì¶œë ¥ (centers_plot ê¸¸ì´ì— ë§ì¶¤)
                                # -----------------------------------------------
                                summary_df = pd.DataFrame({
                                    "center": centers_plot,
                                    "ale_mean": mean_plot,
                                    "ale_lower": lower_plot,
                                    "ale_upper": upper_plot,
                                    "ale_std": std_boot
                                })
                                st.markdown("ë¶€íŠ¸ìŠ¤íŠ¸ë© ê²°ê³¼ ìš”ì•½ (ìœ íš¨ í¬ì¸íŠ¸):")
                                st.dataframe(summary_df)


                                # -----------------------------------------------
                                # 6) ìë™ í•´ì„ ë¦¬í¬íŠ¸ ìƒì„± (ì•ˆì „í•˜ê²Œ centers_plot, mean_plot ì‚¬ìš©)
                                # -----------------------------------------------
                                def interpret_ale_result(feature_name, centers, mean_vals):
                                    """
                                    feature_name: ë¬¸ìì—´
                                    centers: Xì¶• ê°’ (1D array)
                                    mean_vals: ALE í‰ê· ê°’ (1D array, same length)
                                    ë°˜í™˜: (trend, domain_text)
                                    """
                                    # ê¸°ë³¸ ë°©ì–´ ì½”ë“œ
                                    centers = np.asarray(centers)
                                    mean_vals = np.asarray(mean_vals)
                                    if centers.size < 2 or mean_vals.size < 2:
                                        return "ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ íŒ¨í„´ì„ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", ""

                                    # ë³€í™”ìœ¨(ê¸°ìš¸ê¸°) ê³„ì‚° (ì¸ì ‘ êµ¬ê°„ ì°¨ë¶„)
                                    diffs = np.diff(mean_vals)
                                    slope_mean = np.nanmean(diffs)

                                    # ê¸°ìš¸ê¸° í•´ì„ (ë¬¸êµ¬ ìˆ«ì ì„ê³„ê°’ì€ í•„ìš”ì‹œ íŠœë‹)
                                    if slope_mean > 0.5:
                                        trend = "ê°•í•œ ì–‘(+)ì˜ ì˜í–¥ â€” ê°’ì´ ì¦ê°€í• ìˆ˜ë¡ ìˆ˜í™•ëŸ‰ì´ ëšœë ·í•˜ê²Œ ì¦ê°€í•˜ëŠ” íŒ¨í„´"
                                    elif slope_mean > 0.1:
                                        trend = "ì•½í•œ ì–‘(+)ì˜ ì˜í–¥ â€” ê°’ì´ ì¦ê°€í•˜ë©´ ìˆ˜í™•ëŸ‰ì´ ì™„ë§Œí•˜ê²Œ ì¦ê°€"
                                    elif slope_mean < -0.5:
                                        trend = "ê°•í•œ ìŒ(-)ì˜ ì˜í–¥ â€” ê°’ì´ ì¦ê°€í• ìˆ˜ë¡ ìˆ˜í™•ëŸ‰ì´ ëšœë ·í•˜ê²Œ ê°ì†Œí•˜ëŠ” íŒ¨í„´"
                                    elif slope_mean < -0.1:
                                        trend = "ì•½í•œ ìŒ(-)ì˜ ì˜í–¥ â€” ê°’ì´ ì¦ê°€í•˜ë©´ ìˆ˜í™•ëŸ‰ì´ ì™„ë§Œí•˜ê²Œ ê°ì†Œ"
                                    else:
                                        trend = "ìœ ì˜ë¯¸í•œ ë³€í™” ì—†ìŒ â€” ì¦ê°€/ê°ì†Œ ê²½í–¥ì´ ì•½í•¨"

                                    # ìŠ¤ë§ˆíŠ¸íŒœ ë„ë©”ì¸ ê´€ì  ë³´ì¡° í•´ì„ (ë¬¸êµ¬)
                                    fname = feature_name.lower()
                                    domain_text = ""
                                    if "co" in fname or "co2" in fname:
                                        domain_text = ("COâ‚‚ ê´€ë ¨: COâ‚‚ ë†ë„ ì¦ê°€ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê´‘í•©ì„±Â·ìƒì¥ ì´‰ì§„ì„ í†µí•´ ìˆ˜í™•ëŸ‰ì„ "
                                                       "ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆì§€ë§Œ, ê³¼ë„í•œ ë†ë„ì—ì„œëŠ” ì˜¤íˆë ¤ ë¶€ì •ì  ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                    elif "ì˜¨ë„" in feature_name or "temp" in fname:
                                        domain_text = ("ì˜¨ë„ ê´€ë ¨: ì ì • ë²”ìœ„ ë‚´ ì˜¨ë„ ì¦ê°€ëŠ” ìƒì¥ ì´‰ì§„ì„ ìœ ë„í•˜ì§€ë§Œ, ê³¼ì˜¨ ì‹œ ìƒë¦¬ì  "
                                                       "ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë°œìƒí•˜ì—¬ ìˆ˜í™•ëŸ‰ì„ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                    elif "ìŠµ" in feature_name or "hum" in fname:
                                        domain_text = ("ìŠµë„ ê´€ë ¨: ì ì ˆí•œ ìŠµë„ëŠ” ìœ ë¦¬í•˜ë‚˜ ê³¼ìŠµì€ ë³‘í•´ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆê³ , ë„ˆë¬´ ë‚®ìœ¼ë©´ "
                                                       "ê¸°ê³µ ë‹«í˜ìœ¼ë¡œ ê´‘í•©ì„±ì´ ì–µì œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                    elif "ì¼ì‚¬" in feature_name or "solar" in fname or "irradi" in fname:
                                        domain_text = ("ì¼ì‚¬ëŸ‰ ê´€ë ¨: ì¼ì‚¬ëŸ‰ ì¦ê°€ëŠ” ê´‘í•©ì„± ì¦ê°€ë¡œ ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜í™•ëŸ‰ì„ ì˜¬ë¦¬ì§€ë§Œ, "
                                                       "í’ˆì¢…ê³¼ ìƒí™©ì— ë”°ë¼ ê³¼ë‹¤ ì‹œ ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                    else:
                                        domain_text = "ì¼ë°˜ì : ì´ FeatureëŠ” ëª¨ë¸ ì˜ˆì¸¡ì— ì˜í–¥ì´ ìˆìŠµë‹ˆë‹¤."

                                    return trend, domain_text


                                # ì‹¤ì œ í•´ì„ í˜¸ì¶œ
                                trend_text, domain_text = interpret_ale_result(ci_feature, centers_plot, mean_plot)

                                st.markdown("### ğŸ” ALE íŒ¨í„´ ìë™ í•´ì„")
                                st.write(f"**Feature:** {ci_feature}")
                                st.write(f"**ì „ë°˜ì  ê²½í–¥:** {trend_text}")
                                if domain_text:
                                    st.write(f"**ìŠ¤ë§ˆíŠ¸íŒœ ê´€ì  í•´ì„:** {domain_text}")

                                # ------------------------
                                # ì •ëŸ‰ì  íŒ¨í„´ ì¶œë ¥
                                # ------------------------
                                delta = mean_plot[-1] - mean_plot[0]
                                st.markdown("### ğŸ“ˆ ì •ëŸ‰ ìš”ì•½")
                                st.write(f"- ë¶„ì„ êµ¬ê°„ ì „ì²´ ALE ë³€í™”ëŸ‰: **{delta:.3f}**")
                                st.write(f"- í‰ê·  êµ¬ê°„ ê¸°ìš¸ê¸°: **{np.nanmean(np.diff(mean_plot)):.4f}**")
                                st.write(
                                    f"- ìµœëŒ€ ì–‘(+) ì˜í–¥ êµ¬ê°„: center={centers_plot[np.argmax(mean_plot)]:.2f}, ALE={np.max(mean_plot):.3f}")
                                st.write(
                                    f"- ìµœëŒ€ ìŒ(-) ì˜í–¥ êµ¬ê°„: center={centers_plot[np.argmin(mean_plot)]:.2f}, ALE={np.min(mean_plot):.3f}")

                except Exception as e:
                    st.error(f"ALE ë¶€íŠ¸ìŠ¤íŠ¸ë© ì˜¤ë¥˜: {e}")

